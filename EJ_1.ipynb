{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "third-pharmacy",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Administrador\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Administrador\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Administrador\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import pickle\n",
    "import seaborn as sb\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "twenty_train = fetch_20newsgroups(subset='train', shuffle=True, remove=('headers', 'footers'))\n",
    "twenty_test  = fetch_20newsgroups(subset='test' , shuffle=True, remove=('headers', 'footers'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spread-steal",
   "metadata": {},
   "source": [
    "## Revision de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "daily-tuning",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['data', 'filenames', 'target_names', 'target', 'DESCR'])\n",
      "Cantidad de clases:  20\n"
     ]
    }
   ],
   "source": [
    "print(twenty_train.keys())\n",
    "print(\"Cantidad de clases: \",len(twenty_train.target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "leading-argentina",
   "metadata": {},
   "source": [
    "De estos tipos de datos me van a interesar data pq contiene el texto que quiero clasificar y target/target_names que son las clases en las que quiero clasificar los textos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "wireless-karen",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset length: 11314\n"
     ]
    }
   ],
   "source": [
    "data_len = len(twenty_train['data'])\n",
    "print('Dataset length:',data_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "developmental-productivity",
   "metadata": {},
   "source": [
    "Guarde el largo de mi dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "limited-genre",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I was wondering if anyone out there could enlighten me on this car I saw',\n",
       " 'the other day. It was a 2-door sports car, looked to be from the late 60s/',\n",
       " 'early 70s. It was called a Bricklin. The doors were really small. In addition,',\n",
       " 'the front bumper was separate from the rest of the body. This is ',\n",
       " 'all I know. If anyone can tellme a model name, engine specs, years',\n",
       " 'of production, where this car is made, history, or whatever info you',\n",
       " 'have on this funky looking car, please e-mail.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twenty_train['data'][0].split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "humanitarian-commander",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clase 0: 4.2425%\n",
      "Clase 1: 5.1617%\n",
      "Clase 2: 5.2236%\n",
      "Clase 3: 5.2148%\n",
      "Clase 4: 5.1087%\n",
      "Clase 5: 5.2413%\n",
      "Clase 6: 5.1706%\n",
      "Clase 7: 5.2501%\n",
      "Clase 8: 5.2855%\n",
      "Clase 9: 5.2766%\n",
      "Clase 10: 5.3032%\n",
      "Clase 11: 5.2590%\n",
      "Clase 12: 5.2236%\n",
      "Clase 13: 5.2501%\n",
      "Clase 14: 5.2413%\n",
      "Clase 15: 5.2943%\n",
      "Clase 16: 4.8259%\n",
      "Clase 17: 4.9850%\n",
      "Clase 18: 4.1100%\n",
      "Clase 19: 3.3322%\n"
     ]
    }
   ],
   "source": [
    "# Podemos observar la proporcion de clases\n",
    "for i in range(20):\n",
    "    mask = twenty_train.target == i\n",
    "    print(\"Clase {}: {:.4f}%\".format(i , mask.sum()/data_len*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comparable-appendix",
   "metadata": {},
   "source": [
    "Como se puede observar las clases tienen una distribucion aproximadamente uniforme, por lo tanto el accuracy es una buena metrica para evaluar el modelo de prediccion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "likely-bleeding",
   "metadata": {},
   "source": [
    "Una vez con una idea de los datos podemos empezar a procesar los datos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sticky-bulgaria",
   "metadata": {},
   "source": [
    "## Procesamiento de los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proprietary-revision",
   "metadata": {},
   "source": [
    "Vamos a separar estos textos en palabras individuales (tokenizar), lematizar, quitar las palabras que sean muy repetitivas del idioma y reemplazar algunas palabras por su raiz\n",
    "Para esto armamos una funcion la cual perimite elegir cual de los tratamientos aplicar a un determinado set de datos de entrada\n",
    "\n",
    "La lemmatización permite colapsar todas las instancias de una palabra con variaciones en su inflección o modo de uso en un mismo token, reduciendo el conjunto de palabras final y detectando mejor el uso de una misma palabra. En este caso no utilizamos un algoritmo para detectar si palabras con multiples tipos (sustantivo, adjetivo, verbo, adverbio) estan siendo usadas como uno u otro tipo, por lo cual WordNet presume que son sustantivos.\n",
    "\n",
    "Stemming colapsa aún más las instancias de una palabra llevandolas a la raiz de la misma, usualmente resultando en la aproximación a un concepto base. Por ejemplo, se remueven prefijos, sufijos, etc. La lemmatización previa funciona como un stemming más complejo, y el stemmer utilizado termina de reducir las palabras a su forma más simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "intermediate-lying",
   "metadata": {},
   "outputs": [],
   "source": [
    "def treat_data(data, lemm=True, stem=True, stop=True):\n",
    "    lem = WordNetLemmatizer()\n",
    "    stemm = PorterStemmer()\n",
    "    stop_words = stopwords.words('english')\n",
    "    p_data = [word_tokenize(x) for x in data]\n",
    "    print(\"Tokenize ready\")\n",
    "    \n",
    "    if lemm:\n",
    "        p_data = [[lem.lemmatize(y) for y in x ] for x in p_data]\n",
    "        print(\"Lammatize ready\")\n",
    "    if stop:\n",
    "        p_data = [[x for x in y if x not in stop_words] for y in p_data]\n",
    "        print(\"Stop remove ready\")\n",
    "    if stem:\n",
    "        p_data = [[stemm.stem(x) for x in y] for y in p_data]\n",
    "        print(\"Stemmed ready\")\n",
    "    \n",
    "    p_data = [[x for x in y if x.isalpha()] for y in p_data]\n",
    "    print(\"Finished\")\n",
    "    return [' '.join(x) for x in p_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "overall-bosnia",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cache functions\n",
    "def save_data(filename, array):\n",
    "    f = open(filename, 'wb')\n",
    "    pickle.dump(array, f)\n",
    "    f.close()\n",
    "\n",
    "def get_data(filename):\n",
    "    f = open(filename, 'rb')\n",
    "    ret = pickle.load(f)\n",
    "    f.close()\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "terminal-number",
   "metadata": {},
   "source": [
    "Procesamos los datos de train aplicando tokenizacion, lematizacion, stemmer, y sacando las palabras tipicas del lenguaje"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "spanish-railway",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenize ready\n",
      "Lammatize ready\n",
      "Stop remove ready\n",
      "Stemmed ready\n",
      "Finished\n"
     ]
    }
   ],
   "source": [
    "procesado = treat_data(twenty_train['data'])\n",
    "save_data('Cache/procesado_1.pkl', procesado)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surface-valve",
   "metadata": {},
   "source": [
    "Para continuar con el analisis de los datos vamos a realizar un análisis del document frequency de las palabras, esto sera de utilidad para definir rangos posibles para los valores de max_df y min_df a la hora de armar el vocabulario del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "proper-austria",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorizer(data, cv, M_df = 1.0, m_df = 1):\n",
    "    #instancio un count vectorizer para pasar los strings a vectores\n",
    "    vector = cv(max_df=M_df, min_df=m_df)\n",
    "    # Armo el vocabulario\n",
    "    vector.fit(data)\n",
    "    print(\"El largo del vocabulario es:\", len(vector.vocabulary_))\n",
    "    return vector, vector.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "preceding-appearance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El largo del vocabulario es: 48681\n"
     ]
    }
   ],
   "source": [
    "matriz, X_train = vectorizer(procesado, CountVectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "auburn-stanford",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df(sparse_matrix):\n",
    "    times = np.array([x.count_nonzero() for x in sparse_matrix.transpose()])\n",
    "    times_r = times/sparse_matrix.shape[0]\n",
    "    return times, times_r\n",
    "\n",
    "def plot_docfreq(absolute, relative, max_a=100, max_r=0.8):\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(14, 7))\n",
    "    print(\"Max document frequency: {}\\nMin appearances: {}\".format(max(relative), min(absolute)))\n",
    "    axs[0].hist(absolute, bins=20, range=[0, max_a]) #creamos el gráfico en Seaborn\n",
    "    axs[1].hist(relative, bins=20, range=[0, max_r]) #creamos el gráfico en Seaborn\n",
    "    plt.ylabel(\"# Palabras\")\n",
    "    plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "following-finger",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_t, test_tr = get_df(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "reserved-reflection",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max document frequency: 0.5349124977903482\n",
      "Min appearances: 1\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_docfreq(test_t, test_tr, 20, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "comic-spouse",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I wa wonder anyon could enlighten car I saw day It wa sport car look late earli It wa call bricklin the door realli small In addit front bumper wa separ rest bodi thi I know If anyon tellm model name engin spec year product car made histori whatev info funki look car pleas'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "procesado[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "vanilla-startup",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.1757769971857606"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(test_tr > 0.01).sum()/len(test_tr)*100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "transparent-thunder",
   "metadata": {},
   "source": [
    "Se puede observar que menos hay pocas palabras que aparezcan en más del 1% de los articulos, con lo cual se puede tener una pauta de en torno a que valores variar el max_df, y ademas se puede apreicar que la gran mayoria de palabras aparecen en pocos articulos (1 o 2) por lo tanto tambien es posible inferir valores para el min_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "printable-synthetic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23867\n"
     ]
    }
   ],
   "source": [
    "#instancio un count vectorizer para pasar los strings a vectores\n",
    "vector_2 = CountVectorizer(max_df=0.01, min_df=2)\n",
    "# Armo el vocabulario\n",
    "vector_2.fit(procesado)\n",
    "# Paso el volcabulario a matriz y cuenta la cantidad de veces que aparecen port articulo\n",
    "X_train_2 = vector_2.transform(procesado)\n",
    "print(len(vector_2.vocabulary_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outer-morgan",
   "metadata": {},
   "source": [
    "## Smothing Laplaciano\n",
    "\n",
    "Si el vocabulario es muy extenso, las palabras pueden no aparecer nunca para articulos de una determinada clase lo cual se traduce en que P($w_i$|y=k) = 0, esto lleva a que si en un articulo aparece dicha palabra la clase queda totalmente descartada. Para evitar este comportamiento abrupto es posible \"agregar un aticulo\" para cada clase que contenga $\\alpha$ veces cada palabra, evitando asi que niguna likelihood tome valor 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hollow-particle",
   "metadata": {},
   "source": [
    "### Training del modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confidential-batman",
   "metadata": {},
   "source": [
    "Armo una funcion para entrenar y otra para testear el modelo\n",
    "Para el entrenamineto calculo las log_probabilidades condicionales de cada palabra sabiendo en que tipo de articulo estoy trabajando, esto facilita luego los calculos ya que pueden hacerse como producto de matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "informed-shock",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(x_train, y_train, alpha=0):\n",
    "    dl = y_train.shape[0]\n",
    "    lprob, llike = [], []\n",
    "    for i in range(20):\n",
    "        mask = y_train == i\n",
    "        ocurr = x_train[mask, :].sum(axis=0)+alpha\n",
    "        total = ocurr.sum(axis=1).A[0][0]\n",
    "        llike.append(np.log(ocurr/total).A[0])\n",
    "        lprob.append(np.log(mask.sum()/dl))\n",
    "    return np.array(llike), np.array(lprob)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "listed-powder",
   "metadata": {},
   "source": [
    "Para el calculo de las probabilidades a posteriori, al utilizar un modelo de Naive Bayes, asumimos independencia entre palabras, es decir no importa el orden en que aparezcan las mismas ni su lugar en la oración, por lo tanto la formula para la probabilidad queda descripta como:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "instructional-exclusive",
   "metadata": {},
   "source": [
    "$P(y=k|x_1,x_2,...,x_n)=\\prod_{i=1}^{i=n}P(x_i|y=k)\\cdot P(y=k) / P(x_1,..., x_n)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collectible-possibility",
   "metadata": {},
   "source": [
    "$logP(y=k|x_1,x_2,...,x_n)=\\sum_{i=1}^{i=n}logP(x_i|y=k) + logP(y=k) - P(x_1,..., x_n)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "thorough-looking",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(log_l, log_p, x_test, y_test):\n",
    "    corrects = 0\n",
    "    loglike = log_l.transpose()\n",
    "    for i in range(x_test.shape[0]):\n",
    "        res = x_test[i]*loglike + log_p\n",
    "        if res.argmax()==y_test[i]:\n",
    "            corrects += 1\n",
    "    return corrects/x_test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dental-brand",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para porbar el correcto funcionamiento de las funciones pruebo con un alpha arbitrario\n",
    "a = 1\n",
    "log_like, log_prob = train(X_train_2, twenty_train.target, a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "perceived-following",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenize ready\n",
      "Lammatize ready\n",
      "Stop remove ready\n",
      "Stemmed ready\n",
      "Finished\n"
     ]
    }
   ],
   "source": [
    "test_data = treat_data(twenty_test)\n",
    "save_data('Cache/test_1.pkl', test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "southern-lecture",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilizando el count vectorizer que ya sabe el vocabulario, pasamos los datos a matriz\n",
    "X_test = vector_2.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "nutritional-definition",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.2\n"
     ]
    }
   ],
   "source": [
    "aux = test(log_like, log_prob, X_test, twenty_test.target)\n",
    "print(\"Accuracy:\", aux)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "closing-indicator",
   "metadata": {},
   "source": [
    "En esta primera Iteracion se obtuvo una presicion del 71%, por lo cual consideramos que el modelo desarrollado en principio es aceptable pero puede ser mejorado.\n",
    "\n",
    "Para poder mejorar el modelo primero separamos el set de train, en train y validacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "floating-grant",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El largo del vocabulario es: 20851\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(procesado, twenty_train.target, test_size = 0.2, random_state=2)\n",
    "vector, x_train_b = vectorizer(x_train, CountVectorizer, 0.01, 2)\n",
    "x_test_b = vector.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fifteen-relationship",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using aplha = 1, accuracy = 0.7989394608926205\n",
      "Using aplha = 0.1, accuracy = 0.8121961997348652\n",
      "Using aplha = 0.01, accuracy = 0.8121961997348652\n",
      "Using aplha = 0.001, accuracy = 0.8042421564295184\n",
      "Using aplha = 0.0001, accuracy = 0.7971718957136544\n"
     ]
    }
   ],
   "source": [
    "# Voy a variar alpha entre 1 y 0.0001 de forma exponencial\n",
    "for i in range(5):\n",
    "    alpha = 10**(-i)\n",
    "    log_like, log_prob = train(x_train_b, y_train, alpha)\n",
    "    acc = test(log_like, log_prob, x_test_b, y_test)\n",
    "    print(\"Using aplha = {}, accuracy = {}\".format(alpha, acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "immune-number",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using aplha = 0.05, accuracy = 0.8148475475033142\n",
      "Using aplha = 0.060000000000000005, accuracy = 0.8139637649138312\n",
      "Using aplha = 0.07, accuracy = 0.8148475475033142\n",
      "Using aplha = 0.08, accuracy = 0.8139637649138312\n",
      "Using aplha = 0.09, accuracy = 0.8144056562085726\n",
      "Using aplha = 0.1, accuracy = 0.8121961997348652\n",
      "Using aplha = 0.11, accuracy = 0.8117543084401238\n",
      "Using aplha = 0.12000000000000001, accuracy = 0.8130799823243482\n",
      "Using aplha = 0.13, accuracy = 0.8130799823243482\n",
      "Using aplha = 0.14, accuracy = 0.8121961997348652\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    alpha = 0.05+0.01*i\n",
    "    log_like, log_prob = train(x_train_b, y_train, alpha)\n",
    "    acc = test(log_like, log_prob, x_test_b, y_test)\n",
    "    print(\"Using aplha = {}, accuracy = {}\".format(alpha, acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sunrise-operator",
   "metadata": {},
   "source": [
    "Despues de estas dos pruebas podemos considerear el $\\alpha_{optimo} = 0.06$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outside-withdrawal",
   "metadata": {},
   "source": [
    "Ahora relizaremos las mismas pruebas pero variando el min_df y luego el max_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "limiting-details",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(procesado, twenty_train.target, test_size = 0.2, random_state = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "conservative-appraisal",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El largo del vocabulario es: 41450\n",
      "Using min_df = 1, accuracy = 0.8258948298718515\n",
      "El largo del vocabulario es: 20851\n",
      "Using min_df = 2, accuracy = 0.8139637649138312\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,3):\n",
    "    vector, x_train_b = vectorizer(x_train, CountVectorizer, 0.01, i)\n",
    "    x_test_b = vector.transform(x_test)\n",
    "    log_like, log_prob = train(x_train_b, y_train, 0.06)\n",
    "    acc = test(log_like, log_prob, x_test_b, y_test)\n",
    "    print(\"Using min_df = {}, accuracy = {}\".format(i, acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "comparable-manufacturer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El largo del vocabulario es: 20599\n",
      "Using max_df = 1, accuracy = 0.3592576226248343\n",
      "El largo del vocabulario es: 42908\n",
      "Using max_df = 0.1, accuracy = 0.8643393725143614\n",
      "El largo del vocabulario es: 41450\n",
      "Using max_df = 0.01, accuracy = 0.8418029164825453\n",
      "El largo del vocabulario es: 36014\n",
      "Using max_df = 0.001, accuracy = 0.6995139195757843\n"
     ]
    }
   ],
   "source": [
    "for i in range(4):\n",
    "    max_df = 10**(-i)\n",
    "    vector, x_train_b = vectorizer(x_train, TfidfVectorizer, max_df, 1)\n",
    "    x_test_b = vector.transform(x_test)\n",
    "    log_like, log_prob = train(x_train_b, y_train, 0.06)\n",
    "    acc = test(log_like, log_prob, x_test_b, y_test)\n",
    "    print(\"Using max_df = {}, accuracy = {}\".format(max_df, acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "pressed-passing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El largo del vocabulario es: 41450\n",
      "Using max_df = 0.01, accuracy = 0.8418029164825453\n",
      "El largo del vocabulario es: 42443\n",
      "Using max_df = 0.03, accuracy = 0.856827220503756\n",
      "El largo del vocabulario es: 42723\n",
      "Using max_df = 0.05, accuracy = 0.865665046398586\n",
      "El largo del vocabulario es: 42838\n",
      "Using max_df = 0.06999999999999999, accuracy = 0.8643393725143614\n",
      "El largo del vocabulario es: 42887\n",
      "Using max_df = 0.09, accuracy = 0.8643393725143614\n",
      "El largo del vocabulario es: 42928\n",
      "Using max_df = 0.11, accuracy = 0.8647812638091029\n"
     ]
    }
   ],
   "source": [
    "for i in range(6):\n",
    "    max_df = 0.01 + 0.02*i\n",
    "    vector, x_train_b = vectorizer(x_train, TfidfVectorizer, max_df, 1)\n",
    "    x_test_b = vector.transform(x_test)\n",
    "    log_like, log_prob = train(x_train_b, y_train, 0.06)\n",
    "    acc = test(log_like, log_prob, x_test_b, y_test)\n",
    "    print(\"Using max_df = {}, accuracy = {}\".format(max_df, acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pretty-token",
   "metadata": {},
   "source": [
    "De estas pruebas podemos obtener que los mejores resultados se obtuvieron con max_df = 0.03 y min_df = 1.\n",
    "Entrenamos una vez mas el modelo con estos parametros definidos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "invalid-vitamin",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El largo del vocabulario es: 48124\n",
      "Using Tf-idf, accuracy = 0.2\n"
     ]
    }
   ],
   "source": [
    "vector, x_train = vectorizer(procesado, CountVectorizer, 0.03, 1)\n",
    "x_test = vector.transform(test_data)\n",
    "y_train = twenty_train.target\n",
    "y_test = twenty_test.target\n",
    "log_like, log_prob = train(x_train, y_train, 0.06)\n",
    "acc = test(log_like, log_prob, x_test, y_test)\n",
    "print(\"Using Tf-idf, accuracy = {}\".format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regulation-attribute",
   "metadata": {},
   "source": [
    "Otra posibilidad para mejorar el modelo es utilizar tf-idf vectorizer, que ademas de calcular la frecuencia de cada palbra, la multiplica por la inversa del document frequency de la pablabra: \n",
    "$$ tf-idf(palabra, doc) = tf(palabra, doc) *  log [\\frac{ n }{ df(palabra)} ] $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "waiting-frank",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El largo del vocabulario es: 48124\n",
      "Using Tf-idf, accuracy = 0.0\n"
     ]
    }
   ],
   "source": [
    "vector, x_train = vectorizer(procesado, TfidfVectorizer, 0.03, 1)\n",
    "x_test = vector.transform(test_data)\n",
    "y_train = twenty_train.target\n",
    "y_test = twenty_test.target\n",
    "log_like, log_prob = train(x_train, y_train, 0.06)\n",
    "acc = test(log_like, log_prob, x_test, y_test)\n",
    "print(\"Using Tf-idf, accuracy = {}\".format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "defensive-breath",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenize ready\n",
      "Stop remove ready\n",
      "Finished\n",
      "Tokenize ready\n",
      "Stop remove ready\n",
      "Finished\n"
     ]
    }
   ],
   "source": [
    "train_stop = treat_data(twenty_train['data'], lemm=False, stem=False, stop=True)\n",
    "save_data('Cache/train_stop.pkl', train_stop)\n",
    "test_stop = treat_data(twenty_test['data'], lemm=False, stem=False, stop=True)\n",
    "save_data('Cache/test_stop.pkl', test_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "assigned-abuse",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I wa wonder anyon could enlighten car I saw day It wa sport car look late earli It wa call bricklin the door realli small In addit front bumper wa separ rest bodi thi I know If anyon tellm model name engin spec year product car made histori whatev info funki look car pleas\n",
      "I wondering anyone could enlighten car I saw day It sports car looked late early It called Bricklin The doors really small In addition front bumper separate rest body This I know If anyone tellme model name engine specs years production car made history whatever info funky looking car please\n"
     ]
    }
   ],
   "source": [
    "print(procesado[0])\n",
    "print(train_stop[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "aging-houston",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El largo del vocabulario es: 65839\n",
      "Using Tf-idf + only remove stopwords, accuracy = 0.7858470525756771\n"
     ]
    }
   ],
   "source": [
    "vector, x_train = vectorizer(train_stop, TfidfVectorizer, 0.03, 1)\n",
    "x_test = vector.transform(test_stop)\n",
    "y_train = twenty_train.target\n",
    "y_test = twenty_test.target\n",
    "log_like, log_prob = train(x_train, y_train, 0.06)\n",
    "acc = test(log_like, log_prob, x_test, y_test)\n",
    "print(\"Using Tf-idf + only remove stopwords, accuracy = {}\".format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "shaped-technique",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenize ready\n",
      "Lammatize ready\n",
      "Stop remove ready\n",
      "Finished\n",
      "Tokenize ready\n",
      "Lammatize ready\n",
      "Stop remove ready\n",
      "Finished\n"
     ]
    }
   ],
   "source": [
    "train_lemm = treat_data(twenty_train['data'], lemm=True, stem=False, stop=True)\n",
    "save_data('Cache/train_lemm.pkl', train_lemm)\n",
    "test_lemm = treat_data(twenty_test['data'], lemm=True, stem=False, stop=True)\n",
    "save_data('Cache/test_lemm.pkl', test_lemm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "scenic-glass",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El largo del vocabulario es: 62609\n",
      "Using Tf-idf + only remove stopwords, accuracy = 0.77947424322889\n"
     ]
    }
   ],
   "source": [
    "vector, x_train = vectorizer(train_lemm, TfidfVectorizer, 0.03, 1)\n",
    "x_test = vector.transform(test_lemm)\n",
    "y_train = twenty_train.target\n",
    "y_test = twenty_test.target\n",
    "log_like, log_prob = train(x_train, y_train, 0.06)\n",
    "acc = test(log_like, log_prob, x_test, y_test)\n",
    "print(\"Using Tf-idf + only remove stopwords, accuracy = {}\".format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rolled-mention",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
