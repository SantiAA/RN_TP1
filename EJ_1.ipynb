{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fifteen-acrobat",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Administrador\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Administrador\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Administrador\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import pickle\n",
    "import seaborn as sb\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "twenty_train = fetch_20newsgroups(subset='train', shuffle=True, remove=('headers', 'footers'))\n",
    "twenty_test  = fetch_20newsgroups(subset='test' , shuffle=True, remove=('headers', 'footers'))\n",
    "\n",
    "# Separo en train test y validacion\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(twenty_train['data'], twenty_train.target, test_size = 0.2, random_state=2)\n",
    "x_test, y_test = twenty_test['data'], twenty_test.target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expected-companion",
   "metadata": {},
   "source": [
    "## Revision de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "sophisticated-romantic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['data', 'filenames', 'target_names', 'target', 'DESCR'])\n",
      "Cantidad de clases:  20\n"
     ]
    }
   ],
   "source": [
    "print(twenty_train.keys())\n",
    "print(\"Cantidad de clases: \",len(twenty_train.target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "consolidated-highland",
   "metadata": {},
   "source": [
    "De estos tipos de datos me van a interesar data pq contiene el texto que quiero clasificar y target/target_names que son las clases en las que quiero clasificar los textos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "through-flooring",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset length: 11314\n"
     ]
    }
   ],
   "source": [
    "data_len = len(twenty_train['data'])\n",
    "print('Dataset length:',data_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "geological-punishment",
   "metadata": {},
   "source": [
    "Guarde el largo de mi dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "spiritual-ministry",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I was wondering if anyone out there could enlighten me on this car I saw',\n",
       " 'the other day. It was a 2-door sports car, looked to be from the late 60s/',\n",
       " 'early 70s. It was called a Bricklin. The doors were really small. In addition,',\n",
       " 'the front bumper was separate from the rest of the body. This is ',\n",
       " 'all I know. If anyone can tellme a model name, engine specs, years',\n",
       " 'of production, where this car is made, history, or whatever info you',\n",
       " 'have on this funky looking car, please e-mail.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twenty_train['data'][0].split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "essential-wheat",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clase 0: 4.2425%\n",
      "Clase 1: 5.1617%\n",
      "Clase 2: 5.2236%\n",
      "Clase 3: 5.2148%\n",
      "Clase 4: 5.1087%\n",
      "Clase 5: 5.2413%\n",
      "Clase 6: 5.1706%\n",
      "Clase 7: 5.2501%\n",
      "Clase 8: 5.2855%\n",
      "Clase 9: 5.2766%\n",
      "Clase 10: 5.3032%\n",
      "Clase 11: 5.2590%\n",
      "Clase 12: 5.2236%\n",
      "Clase 13: 5.2501%\n",
      "Clase 14: 5.2413%\n",
      "Clase 15: 5.2943%\n",
      "Clase 16: 4.8259%\n",
      "Clase 17: 4.9850%\n",
      "Clase 18: 4.1100%\n",
      "Clase 19: 3.3322%\n"
     ]
    }
   ],
   "source": [
    "# Podemos observar la proporcion de clases\n",
    "for i in range(20):\n",
    "    mask = twenty_train.target == i\n",
    "    print(\"Clase {}: {:.4f}%\".format(i , mask.sum()/data_len*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proud-speaker",
   "metadata": {},
   "source": [
    "Como se puede observar las clases tienen una distribucion aproximadamente uniforme, por lo tanto el accuracy es una buena metrica para evaluar el modelo de prediccion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "trained-tulsa",
   "metadata": {},
   "source": [
    "Una vez con una idea de los datos podemos empezar a procesar los datos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "australian-reach",
   "metadata": {},
   "source": [
    "## Procesamiento de los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intermediate-reach",
   "metadata": {},
   "source": [
    "Vamos a separar estos textos en palabras individuales (tokenizar), lematizar, quitar las palabras que sean muy repetitivas del idioma y reemplazar algunas palabras por su raiz\n",
    "Para esto armamos una funcion la cual perimite elegir cual de los tratamientos aplicar a un determinado set de datos de entrada\n",
    "\n",
    "La lemmatización permite colapsar todas las instancias de una palabra con variaciones en su inflección o modo de uso en un mismo token, reduciendo el conjunto de palabras final y detectando mejor el uso de una misma palabra. En este caso no utilizamos un algoritmo para detectar si palabras con multiples tipos (sustantivo, adjetivo, verbo, adverbio) estan siendo usadas como uno u otro tipo, por lo cual WordNet presume que son sustantivos.\n",
    "\n",
    "Stemming colapsa aún más las instancias de una palabra llevandolas a la raiz de la misma, usualmente resultando en la aproximación a un concepto base. Por ejemplo, se remueven prefijos, sufijos, etc. La lemmatización previa funciona como un stemming más complejo, y el stemmer utilizado termina de reducir las palabras a su forma más simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "laughing-wednesday",
   "metadata": {},
   "outputs": [],
   "source": [
    "def treat_data(data, lemm=True, stem=True, stop=True):\n",
    "    lem = WordNetLemmatizer()\n",
    "    stemm = PorterStemmer()\n",
    "    stop_words = stopwords.words('english')\n",
    "    p_data = [word_tokenize(x) for x in data]\n",
    "    print(\"Tokenize ready\")\n",
    "    \n",
    "    if lemm:\n",
    "        p_data = [[lem.lemmatize(y) for y in x ] for x in p_data]\n",
    "        print(\"Lammatize ready\")\n",
    "    if stop:\n",
    "        p_data = [[x for x in y if x not in stop_words] for y in p_data]\n",
    "        print(\"Stop remove ready\")\n",
    "    if stem:\n",
    "        p_data = [[stemm.stem(x) for x in y] for y in p_data]\n",
    "        print(\"Stemmed ready\")\n",
    "    \n",
    "    p_data = [[x for x in y if x.isalpha()] for y in p_data]\n",
    "    print(\"Finished\")\n",
    "    return [' '.join(x) for x in p_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "german-anxiety",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cache functions\n",
    "def save_data(filename, array):\n",
    "    f = open(filename, 'wb')\n",
    "    pickle.dump(array, f)\n",
    "    f.close()\n",
    "\n",
    "def get_data(filename):\n",
    "    f = open(filename, 'rb')\n",
    "    ret = pickle.load(f)\n",
    "    f.close()\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loving-recipe",
   "metadata": {},
   "source": [
    "Procesamos los datos de train aplicando tokenizacion, lematizacion, stemmer, y sacando las palabras tipicas del lenguaje"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "average-market",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenize ready\n",
      "Lammatize ready\n",
      "Stop remove ready\n",
      "Stemmed ready\n",
      "Finished\n",
      "Tokenize ready\n",
      "Lammatize ready\n",
      "Stop remove ready\n",
      "Stemmed ready\n",
      "Finished\n"
     ]
    }
   ],
   "source": [
    "x_train_p = treat_data(x_train)\n",
    "x_valid_p = treat_data(x_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "transsexual-brisbane",
   "metadata": {},
   "source": [
    "Para continuar con el analisis de los datos vamos a realizar un análisis del document frequency de las palabras, esto sera de utilidad para definir rangos posibles para los valores de max_df y min_df a la hora de armar el vocabulario del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "educated-unknown",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorizer(data, cv, M_df = 1.0, m_df = 1):\n",
    "    #instancio un count vectorizer para pasar los strings a vectores\n",
    "    vector = cv(max_df=M_df, min_df=m_df)\n",
    "    # Armo el vocabulario\n",
    "    vector.fit(data)\n",
    "    print(\"El largo del vocabulario es:\", len(vector.vocabulary_))\n",
    "    return vector, vector.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "severe-japan",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El largo del vocabulario es: 43007\n"
     ]
    }
   ],
   "source": [
    "train_matrix, x_train_v = vectorizer(x_train_p, CountVectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "offshore-university",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df(sparse_matrix):\n",
    "    times = np.array([x.count_nonzero() for x in sparse_matrix.transpose()])\n",
    "    times_r = times/sparse_matrix.shape[0]\n",
    "    return times, times_r\n",
    "\n",
    "def plot_docfreq(absolute, relative, max_a=100, max_r=0.8):\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(14, 7))\n",
    "    print(\"Max document frequency: {}\\nMin appearances: {}\".format(max(relative), min(absolute)))\n",
    "    axs[0].hist(absolute, bins=20, range=[0, max_a]) #creamos el gráfico en Seaborn\n",
    "    axs[1].hist(relative, bins=20, range=[0, max_r]) #creamos el gráfico en Seaborn\n",
    "    plt.ylabel(\"# Palabras\")\n",
    "    plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "chronic-berry",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_t, test_tr = get_df(x_train_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "advanced-thousand",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max document frequency: 0.5333112363274777\n",
      "Min appearances: 1\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_docfreq(test_t, test_tr, 20, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "trained-combine",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i look good deal follow use new aviat headset mic handheld tranciev may consid com portabl gp loran navig repli call pleas wake night worker hous must call day i understand want employ pay call jay snyder jay gdx uunet compnect gdx jay thi brain unix worldblaz thi brain drug msdo unix msdo file area xenix bin'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_p[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "controlling-screening",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.622666077615272"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(test_tr > 0.01).sum()/len(test_tr)*100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extreme-casino",
   "metadata": {},
   "source": [
    "Se puede observar que menos hay pocas palabras que aparezcan en más del 1% de los articulos, con lo cual se puede tener una pauta de en torno a que valores variar el max_df, y ademas se puede apreicar que la gran mayoria de palabras aparecen en pocos articulos (1 o 2) por lo tanto tambien es posible inferir valores para el min_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bacterial-cement",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20851\n"
     ]
    }
   ],
   "source": [
    "#instancio un count vectorizer para pasar los strings a vectores\n",
    "c_vector = CountVectorizer(max_df=0.01, min_df=2)\n",
    "# Armo el vocabulario\n",
    "c_vector.fit(x_train_p)\n",
    "# Paso el volcabulario a matriz y cuenta la cantidad de veces que aparecen port articulo\n",
    "x_train_Mdf_mdf = c_vector.transform(x_train_p)\n",
    "print(len(c_vector.vocabulary_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vocational-receptor",
   "metadata": {},
   "source": [
    "## Smothing Laplaciano\n",
    "\n",
    "Si el vocabulario es muy extenso, las palabras pueden no aparecer nunca para articulos de una determinada clase lo cual se traduce en que P($w_i$|y=k) = 0, esto lleva a que si en un articulo aparece dicha palabra la clase queda totalmente descartada. Para evitar este comportamiento abrupto es posible \"agregar un aticulo\" para cada clase que contenga $\\alpha$ veces cada palabra, evitando asi que niguna likelihood tome valor 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "impressed-aspect",
   "metadata": {},
   "source": [
    "### Training del modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "refined-cable",
   "metadata": {},
   "source": [
    "Armo una funcion para entrenar y otra para testear el modelo\n",
    "Para el entrenamineto calculo las log_probabilidades condicionales de cada palabra sabiendo en que tipo de articulo estoy trabajando, esto facilita luego los calculos ya que pueden hacerse como producto de matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "smaller-conjunction",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(x_train, y_train, alpha=0):\n",
    "    dl = y_train.shape[0]\n",
    "    lprob, llike = [], []\n",
    "    for i in range(20):\n",
    "        mask = y_train == i\n",
    "        ocurr = x_train[mask, :].sum(axis=0)+alpha\n",
    "        total = ocurr.sum(axis=1).A[0][0]\n",
    "        llike.append(np.log(ocurr/total).A[0])\n",
    "        lprob.append(np.log(mask.sum()/dl))\n",
    "    return np.array(llike), np.array(lprob)\n",
    "\n",
    "def test(log_l, log_p, x_test, y_test):\n",
    "    corrects = 0\n",
    "    loglike = log_l.transpose()\n",
    "    for i in range(x_test.shape[0]):\n",
    "        res = x_test[i]*loglike + log_p\n",
    "        if res.argmax()==y_test[i]:\n",
    "            corrects += 1\n",
    "    return corrects/x_test.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "small-choir",
   "metadata": {},
   "source": [
    "Para el calculo de las probabilidades a posteriori, al utilizar un modelo de Naive Bayes, asumimos independencia entre palabras, es decir no importa el orden en que aparezcan las mismas ni su lugar en la oración, por lo tanto la formula para la probabilidad queda descripta como:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "typical-implementation",
   "metadata": {},
   "source": [
    "$P(y=k|x_1,x_2,...,x_n)=\\prod_{i=1}^{i=n}P(x_i|y=k)\\cdot P(y=k) / P(x_1,..., x_n)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pleased-startup",
   "metadata": {},
   "source": [
    "$logP(y=k|x_1,x_2,...,x_n)=\\sum_{i=1}^{i=n}logP(x_i|y=k) + logP(y=k) - P(x_1,..., x_n)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "confidential-citation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.908960335874489\n",
      "Validation Accuracy: 0.7989394608926205\n"
     ]
    }
   ],
   "source": [
    "# Para porbar el correcto funcionamiento de las funciones pruebo con un alpha arbitrario\n",
    "a = 1\n",
    "log_like, log_prob = train(x_train_Mdf_mdf, y_train, a)\n",
    "# Utilizando el count vectorizer que ya sabe el vocabulario, pasamos los datos a matriz\n",
    "x_valid_Mdf_mdf = c_vector.transform(x_valid_p)\n",
    "# Verifico el funcionamient\n",
    "acc_train = test(log_like, log_prob, x_train_Mdf_mdf, y_train) \n",
    "acc_valid = test(log_like, log_prob, x_valid_Mdf_mdf, y_valid)\n",
    "print(\"Train Accuracy:\", acc_train)\n",
    "print(\"Validation Accuracy:\", acc_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cheap-complement",
   "metadata": {},
   "source": [
    "Podemos observar un buen comportamiento del modelo, pero tambien cierto grado de overffiting, vamos a continuar mejorando el  modelo para buscar un cambio en estos resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "seasonal-tunnel",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using aplha = 1, train_acc = 0.9090, valid_acc = 0.7989\n",
      "Using aplha = 0.1, train_acc = 0.9333, valid_acc = 0.8122\n",
      "Using aplha = 0.01, train_acc = 0.9439, valid_acc = 0.8122\n",
      "Using aplha = 0.001, train_acc = 0.9470, valid_acc = 0.8042\n",
      "Using aplha = 0.0001, train_acc = 0.9476, valid_acc = 0.7972\n"
     ]
    }
   ],
   "source": [
    "# Voy a variar alpha entre 1 y 0.0001 de forma exponencial\n",
    "for i in range(5):\n",
    "    alpha = 10**(-i)\n",
    "    log_like, log_prob = train(x_train_Mdf_mdf, y_train, alpha)\n",
    "    acc_train = test(log_like, log_prob, x_train_Mdf_mdf, y_train) \n",
    "    acc_valid = test(log_like, log_prob, x_valid_Mdf_mdf, y_valid)\n",
    "    print(\"Using aplha = {}, train_acc = {:.4f}, valid_acc = {:.4f}\".format(alpha, acc_train, acc_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "indie-prescription",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using aplha = 0.05, train_acc = 0.9376, valid_acc = 0.8148\n",
      "Using aplha = 0.06, train_acc = 0.9367, valid_acc = 0.8140\n",
      "Using aplha = 0.07, train_acc = 0.9359, valid_acc = 0.8148\n",
      "Using aplha = 0.08, train_acc = 0.9345, valid_acc = 0.8140\n",
      "Using aplha = 0.09, train_acc = 0.9339, valid_acc = 0.8144\n",
      "Using aplha = 0.10, train_acc = 0.9333, valid_acc = 0.8122\n",
      "Using aplha = 0.11, train_acc = 0.9324, valid_acc = 0.8118\n",
      "Using aplha = 0.12, train_acc = 0.9322, valid_acc = 0.8131\n",
      "Using aplha = 0.13, train_acc = 0.9317, valid_acc = 0.8131\n",
      "Using aplha = 0.14, train_acc = 0.9312, valid_acc = 0.8122\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    alpha = 0.05+0.01*i\n",
    "    log_like, log_prob = train(x_train_Mdf_mdf, y_train, alpha)\n",
    "    acc_train = test(log_like, log_prob, x_train_Mdf_mdf, y_train) \n",
    "    acc_valid = test(log_like, log_prob, x_valid_Mdf_mdf, y_valid)\n",
    "    print(\"Using aplha = {:.2f}, train_acc = {:.4f}, valid_acc = {:.4f}\".format(alpha, acc_train, acc_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expired-angel",
   "metadata": {},
   "source": [
    "Despues de estas dos pruebas podemos considerear el $\\alpha_{optimo} = 0.07$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "functioning-cradle",
   "metadata": {},
   "source": [
    "Ahora relizaremos las mismas pruebas pero variando el min_df y luego el max_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "modular-jacket",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El largo del vocabulario es: 41449\n",
      "Using min_df = 1.00, train_acc = 0.9520, valid_acc = 0.8246\n",
      "El largo del vocabulario es: 20851\n",
      "Using min_df = 2.00, train_acc = 0.9359, valid_acc = 0.8148\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,3):\n",
    "    vector, x_train_b = vectorizer(x_train_p, CountVectorizer, 0.01, i)\n",
    "    x_valid_b = vector.transform(x_valid_p)\n",
    "    log_like, log_prob = train(x_train_b, y_train, 0.07)\n",
    "    acc_train = test(log_like, log_prob, x_train_b, y_train) \n",
    "    acc_valid = test(log_like, log_prob, x_valid_b, y_valid)\n",
    "    print(\"Using min_df = {:.2f}, train_acc = {:.4f}, valid_acc = {:.4f}\".format(i, acc_train, acc_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "suburban-crystal",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El largo del vocabulario es: 20598\n",
      "Using max_df = 1.00, train_acc = 0.5528, valid_acc = 0.3544\n",
      "El largo del vocabulario es: 42907\n",
      "Using max_df = 0.10, train_acc = 0.9555, valid_acc = 0.8542\n",
      "El largo del vocabulario es: 41449\n",
      "Using max_df = 0.01, train_acc = 0.9520, valid_acc = 0.8246\n",
      "El largo del vocabulario es: 36013\n",
      "Using max_df = 0.00, train_acc = 0.8894, valid_acc = 0.6743\n"
     ]
    }
   ],
   "source": [
    "for i in range(4):\n",
    "    max_df = 10**(-i)\n",
    "    vector, x_train_b = vectorizer(x_train_p, CountVectorizer, max_df, 1)\n",
    "    x_valid_b = vector.transform(x_valid_p)\n",
    "    log_like, log_prob = train(x_train_b, y_train, 0.07)\n",
    "    acc_train = test(log_like, log_prob, x_train_b, y_train) \n",
    "    acc_valid = test(log_like, log_prob, x_valid_b, y_valid)\n",
    "    print(\"Using max_df = {:.2f}, train_acc = {:.4f}, valid_acc = {:.4f}\".format(max_df, acc_train, acc_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "modern-finnish",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El largo del vocabulario es: 42722\n",
      "Using max_df = 0.05, train_acc = 0.9562, valid_acc = 0.8529\n",
      "El largo del vocabulario es: 42837\n",
      "Using max_df = 0.07, train_acc = 0.9546, valid_acc = 0.8506\n",
      "El largo del vocabulario es: 42886\n",
      "Using max_df = 0.09, train_acc = 0.9554, valid_acc = 0.8537\n",
      "El largo del vocabulario es: 42927\n",
      "Using max_df = 0.11, train_acc = 0.9555, valid_acc = 0.8546\n",
      "El largo del vocabulario es: 42946\n",
      "Using max_df = 0.13, train_acc = 0.9554, valid_acc = 0.8542\n",
      "El largo del vocabulario es: 42964\n",
      "Using max_df = 0.15, train_acc = 0.9549, valid_acc = 0.8537\n"
     ]
    }
   ],
   "source": [
    "for i in range(6):\n",
    "    max_df = 0.05 + 0.02*i\n",
    "    vector, x_train_b = vectorizer(x_train_p, CountVectorizer, max_df, 1)\n",
    "    x_valid_b = vector.transform(x_valid_p)\n",
    "    log_like, log_prob = train(x_train_b, y_train, 0.07)\n",
    "    acc_train = test(log_like, log_prob, x_train_b, y_train) \n",
    "    acc_valid = test(log_like, log_prob, x_valid_b, y_valid)\n",
    "    print(\"Using max_df = {:.2f}, train_acc = {:.4f}, valid_acc = {:.4f}\".format(max_df, acc_train, acc_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "varied-voice",
   "metadata": {},
   "source": [
    "De estas pruebas podemos obtener que los mejores resultados se obtuvieron con max_df = 0.11y min_df = 1.\n",
    "Entrenamos una vez mas el modelo con estos parametros definidos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "interior-exemption",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El largo del vocabulario es: 42927\n"
     ]
    }
   ],
   "source": [
    "final_c_vector, x_train_d = vectorizer(x_train_p, CountVectorizer, 0.11, 1)\n",
    "x_valid_d = final_c_vector.transform(x_valid_p)\n",
    "# Entreno y guardo para probar despues\n",
    "log_like_1, log_prob_1 = train(x_train_d, y_train, 0.07)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hired-plymouth",
   "metadata": {},
   "source": [
    "Otra posibilidad para mejorar el modelo es utilizar tf-idf vectorizer, que ademas de calcular la frecuencia de cada palbra, la multiplica por la inversa del document frequency de la pablabra: \n",
    "$$ tf-idf(palabra, doc) = tf(palabra, doc) \\cdot  log \\left[\\frac{ n }{ df(palabra)} \\right] $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fifty-pharmacology",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El largo del vocabulario es: 42927\n",
      "Using Tf-Idf, train_acc = 0.9751, valid_acc = 0.8635\n"
     ]
    }
   ],
   "source": [
    "tfidf_vector, x_train_idf = vectorizer(x_train_p, TfidfVectorizer, 0.11, 1)\n",
    "x_valid_idf = tfidf_vector.transform(x_valid_p)\n",
    "log_like_tfidf, log_prob_tfidf = train(x_train_idf, y_train, 0.07)\n",
    "\n",
    "acc_train = test(log_like_tfidf, log_prob_tfidf, x_train_idf, y_train) \n",
    "acc_valid = test(log_like_tfidf, log_prob_tfidf, x_valid_idf, y_valid)\n",
    "print(\"Using Tf-Idf, train_acc = {:.4f}, valid_acc = {:.4f}\".format(acc_train, acc_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1e2924-1785-4d80-bb64-1a15dd3260bf",
   "metadata": {},
   "source": [
    "Ahora se prueban distintas combinanciones de proceamiento del texto original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "former-liverpool",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenize ready\n",
      "Stop remove ready\n",
      "Finished\n",
      "Tokenize ready\n",
      "Stop remove ready\n",
      "Finished\n",
      "El largo del vocabulario es: 59111\n",
      "Using Tf-idf + only remove stopwords, train_acc = 0.9831, valid_acc = 0.8670\n"
     ]
    }
   ],
   "source": [
    "x_train_stop = treat_data(x_train, lemm=False, stem=False, stop=True)\n",
    "x_valid_stop = treat_data(x_valid, lemm=False, stem=False, stop=True)\n",
    "\n",
    "stop_vector, x_train_stop_p = vectorizer(x_train_stop, TfidfVectorizer, 0.11, 1)\n",
    "x_valid_stop_p = stop_vector.transform(x_valid_stop)\n",
    "\n",
    "log_like_stop, log_prob_stop = train(x_train_stop_p, y_train, 0.07)\n",
    "\n",
    "acc_train = test(log_like_stop, log_prob_stop, x_train_stop_p, y_train) \n",
    "acc_valid = test(log_like_stop, log_prob_stop, x_valid_stop_p, y_valid)\n",
    "print(\"Using Tf-idf + only remove stopwords, train_acc = {:.4f}, valid_acc = {:.4f}\".format(acc_train, acc_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "contemporary-following",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenize ready\n",
      "Lammatize ready\n",
      "Stop remove ready\n",
      "Finished\n",
      "Tokenize ready\n",
      "Lammatize ready\n",
      "Stop remove ready\n",
      "Finished\n",
      "El largo del vocabulario es: 56084\n",
      "Using Tf-idf + only remove stopwords, train_acc = 0.9818, valid_acc = 0.8643\n"
     ]
    }
   ],
   "source": [
    "x_train_lemm = treat_data(x_train, lemm=True, stem=False, stop=True)\n",
    "x_valid_lemm = treat_data(x_valid, lemm=True, stem=False, stop=True)\n",
    "\n",
    "lemm_vector, x_train_lemm_p = vectorizer(x_train_lemm, TfidfVectorizer, 0.11, 1)\n",
    "x_valid_lemm_p = lemm_vector.transform(x_valid_lemm)\n",
    "\n",
    "log_like_lemm, log_prob_lemm = train(x_train_lemm_p, y_train, 0.07)\n",
    "\n",
    "acc_train = test(log_like_lemm, log_prob_lemm, x_train_lemm_p, y_train) \n",
    "acc_valid = test(log_like_lemm, log_prob_lemm, x_valid_lemm_p, y_valid)\n",
    "print(\"Using Tf-idf + only remove stopwords, train_acc = {:.4f}, valid_acc = {:.4f}\".format(acc_train, acc_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf51df9-25c7-4c8e-b951-4009be50ca24",
   "metadata": {},
   "source": [
    "## Ahora probamos los dos mejores modelos en test, para obstener una metrica final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e3c430c8-7e52-4414-81ab-cc3e237cdfe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenize ready\n",
      "Lammatize ready\n",
      "Stop remove ready\n",
      "Stemmed ready\n",
      "Finished\n",
      "Using Tf-Idf, TEST_acc = 0.7742\n"
     ]
    }
   ],
   "source": [
    "x_test_p = treat_data(x_test)\n",
    "x_test_idf = tfidf_vector.transform(x_test_p)\n",
    "acc_test = test(log_like_tfidf, log_prob_tfidf, x_test_idf, y_test)\n",
    "print(\"Using Tf-Idf, TEST_acc = {:.4f}\".format(acc_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "45495cf9-92ec-4c9b-8e1a-6ec21ff99457",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenize ready\n",
      "Stop remove ready\n",
      "Finished\n",
      "Using Tf-idf + only remove stopwords, TEST_acc = 0.7801\n"
     ]
    }
   ],
   "source": [
    "x_test_stop = treat_data(x_test, lemm=False, stem=False, stop=True)\n",
    "x_test_stop_p = stop_vector.transform(x_test_stop)\n",
    "acc_test = test(log_like_stop, log_prob_stop, x_test_stop_p, y_test) \n",
    "print(\"Using Tf-idf + only remove stopwords, TEST_acc = {:.4f}\".format(acc_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d787a7-2f55-47e3-a059-29f69c2a1544",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
